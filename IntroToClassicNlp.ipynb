{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'a', 's', 'h']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[a-z]\", \"$34.33 cash.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'phone']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"(name|phone):\", \"My name: Joe, my phone: (312)555-1212\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lion', 'lion']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"([Ll]ion)s?\", \"Give it to the Lions or the lion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hxx. I xxxx xxxx xxxxx xxxxx...'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[a-z]\", \"x\", \"Hey. I know this regex stuff...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gutenberg book ids= ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"gutenberg book ids=\", nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(alice)= 34110\n",
      "['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit', '-', 'Hole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\", 'So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',']\n"
     ]
    }
   ],
   "source": [
    "alice = nltk.corpus.gutenberg.words(\"carroll-alice.txt\")\n",
    "print (\"len(alice)=\", len(alice))\n",
    "print(alice[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(grail)= 16967\n",
      "['SCENE', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop', ']', 'KING', 'ARTHUR', ':', 'Whoa', 'there', '!', '[', 'clop', 'clop', 'clop', ']', 'SOLDIER', '#', '1', ':', 'Halt', '!', 'Who', 'goes', 'there', '?', 'ARTHUR', ':', 'It', 'is', 'I', ',', 'Arthur', ',', 'son', 'of', 'Uther', 'Pendragon', ',', 'from', 'the', 'castle', 'of', 'Camelot', '.', 'King', 'of', 'the', 'Britons', ',', 'defeator', 'of', 'the', 'Saxons', ',', 'sovereign', 'of', 'all', 'England', '!', 'SOLDIER', '#', '1', ':', 'Pull', 'the', 'other', 'one', '!', 'ARTHUR', ':', 'I', 'am', ',', '...', 'and', 'this', 'is', 'my', 'trusty', 'servant', 'Patsy', '.', 'We', 'have', 'ridden', 'the', 'length', 'and', 'breadth', 'of', 'the', 'land', 'in']\n"
     ]
    }
   ],
   "source": [
    "grail = nltk.corpus.webtext.words(\"grail.txt\")\n",
    "print(\"len(grail)=\",len(grail))\n",
    "\n",
    "print(grail[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'man', 'ran', 'after', 'it']\n",
      "['theta', 'bled', 'own', 'there']\n"
     ]
    }
   ],
   "source": [
    "#Plain text extraction\n",
    "def tokenize(str, dict):\n",
    "    s = 0\n",
    "    words = []\n",
    "    \n",
    "    while (s < len(str)):\n",
    "        found = False\n",
    "        \n",
    "        #find biggest word in dict that mathces str[s:xxx]\n",
    "        for word in dict:\n",
    "            lw = len(word)\n",
    "            if (str[s:s+lw] == word):\n",
    "                words.append(word)\n",
    "                s += lw\n",
    "                found = True\n",
    "                break\n",
    "        if (not found):\n",
    "            words.append(str[s])\n",
    "            s += 1\n",
    "                \n",
    "    print(words)\n",
    "    #return words\n",
    "        \n",
    "# small dictionary of known words, longest words first\n",
    "dict = [\"before\", \"table\", \"theta\", \"after\", \"where\", \"there\", \"bled\", \"said\", \"lead\", \"man\", \"her\", \"own\", \"the\", \"ran\", \"it\"]\n",
    "\n",
    "# this algorithm is designed to work with languages that don't have whitspace hcarcters\n",
    "# so simulate that in our test\n",
    "tokenize(\"themanranafterit\", dict)    #works!\n",
    "tokenize(\"thetabledownthere\", dict)  #fails!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'man',\n",
       " ',',\n",
       " 'he',\n",
       " 'ran',\n",
       " 'after',\n",
       " 'it',\n",
       " \"'s\",\n",
       " '$',\n",
       " '3.23',\n",
       " 'dog',\n",
       " 'on',\n",
       " '03/23/2016',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK example: WORD segementation\n",
    "\n",
    "nltk.word_tokenize(\"the man, he ran after it's $3.23 dog on 03/23/2016.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the man ran after it.', 'The table down there?', 'Yes, down there!']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(\"the man ran after it. The table down there? Yes, down there!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test stop word removal code'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stopwords removal\n",
    "\n",
    "stoppers = \"a is of the this\". split()\n",
    "\n",
    "def removeStopWords(stopwords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopwords])\n",
    "    return newtxt\n",
    "removeStopWords(stoppers, \"this is a test of the stop word removal code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(stops)= 179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test stop word removal code.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK example: removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"English\")\n",
    "\n",
    "print(\"len(stops)=\", len(stops))\n",
    "\n",
    "removeStopWords(stops, \"this is a test of the stop word removal code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the man ran after it. the table down there? yes, down there!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#case removal\n",
    "str = \"The man ran after it. The table down there? Yes, down there!\"\n",
    "str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter: ['pleas', \"don't\", 'ubnuckl', 'your', 'seat-belt', 'while', 'I', 'am', 'driving.', 'he', 'said']\n",
      "\n",
      "lancaster: ['pleas', \"don't\", 'ubnuckl', 'yo', 'seat-belt', 'whil', 'i', 'am', 'driving.', 'he', 'said']\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "#NLTK example: stemming\n",
    "\n",
    "def stem_with_porter(words):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words\n",
    "\n",
    "def stem_with_lancaster(words):\n",
    "    porter = nltk.LancasterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words\n",
    "\n",
    "str = \"Please don't ubnuckle your seat-belt while I am driving. he said\"\n",
    "\n",
    "print (\"porter:\", stem_with_porter(str.split()))\n",
    "print()\n",
    "print (\"lancaster:\", stem_with_lancaster(str.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
